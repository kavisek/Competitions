{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T22:02:07.117454Z",
     "start_time": "2018-04-23T22:02:06.817232Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "\n",
    "\n",
    "# Prep Sample File Function\n",
    "def prep_sample_submission (model_proba):\n",
    "    model_proba['project_is_approved'] = 0\n",
    "    for index, row in model_proba.iterrows():\n",
    "        if model_proba.loc[index,0] > model_proba.loc[index,0]:\n",
    "            model_proba.loc[index,'project_is_approved'] = model_proba.loc[index,0]\n",
    "        else:\n",
    "            model_proba.loc[index,'project_is_approved'] = model_proba.loc[index,1]\n",
    "    return model_proba\n",
    "\n",
    "pd.set_option('max_columns',1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T21:58:45.373197Z",
     "start_time": "2018-04-23T21:58:40.006193Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Data\n",
    "test_data = pd.read_csv('Data/test.csv', low_memory=False)\n",
    "train_data = pd.read_csv('Data/train.csv', low_memory=False)\n",
    "\n",
    "train_df_classes = train_data['project_is_approved']\n",
    "\n",
    "# Training Features\n",
    "features = ['teacher_prefix', 'school_state', 'project_grade_category', 'project_subject_categories', 'project_subject_subcategories',\n",
    "            'teacher_number_of_previously_posted_projects']\n",
    "\n",
    "# Training and Testing Dataframes\n",
    "train_df = train_data[features]\n",
    "test_df = test_data[features]\n",
    "train_df = train_df.fillna(value='No Essay')\n",
    "test_df = test_df.fillna(value='No Essay')\n",
    "test_ids = test_data.id\n",
    "\n",
    "#Encoding Values\n",
    "for col in list(train_df.select_dtypes('object').columns):\n",
    "    train_df[col] = train_df[col].astype('category')\n",
    "    train_df[col] = train_df[col].cat.codes\n",
    "    \n",
    "for col in list(test_df.select_dtypes('object').columns):\n",
    "    test_df[col] = test_df[col].astype('category')\n",
    "    test_df[col] = test_df[col].cat.codes\n",
    "    \n",
    "y = train_df_classes.values\n",
    "X = train_df.values\n",
    "X_test = test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taining Data\n",
    "essay_features = ['project_essay_1','project_essay_2','project_essay_3','project_essay_4','project_resource_summary']\n",
    "\n",
    "train_edf = train_data[essay_features]\n",
    "test_edf = test_data[essay_features]\n",
    "\n",
    "# Filter Essay Features\n",
    "train_edf = train_data[essay_features]\n",
    "test_edf = test_data[essay_features]\n",
    "\n",
    "# Fill DataFrame\n",
    "train_edf = train_edf.fillna(value='No Essay')\n",
    "test_edf = test_edf.fillna(value='No Essay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logisical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T21:59:26.623428Z",
     "start_time": "2018-04-23T21:58:58.552524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folds: 10,accuracy: 0.85 std: 0.00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>project_is_approved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p233245</td>\n",
       "      <td>0.837230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p096795</td>\n",
       "      <td>0.847115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p236235</td>\n",
       "      <td>0.808259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p233680</td>\n",
       "      <td>0.836270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p171879</td>\n",
       "      <td>0.854131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  project_is_approved\n",
       "0  p233245             0.837230\n",
       "1  p096795             0.847115\n",
       "2  p236235             0.808259\n",
       "3  p233680             0.836270\n",
       "4  p171879             0.854131"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Logistic Model\n",
    "model = LogisticRegression()\n",
    "model.fit(X,y)\n",
    "\n",
    "# Predict Sample Probabilities\n",
    "logpred = pd.Series(model.predict(X_test), name='project_is_approved_result')\n",
    "logpred_proba_train = pd.DataFrame(model.predict_proba(X))\n",
    "logpred_proba_test = pd.DataFrame(model.predict_proba(X_test))\n",
    "logpred_sample_file = prep_sample_submission(logpred_proba_test)\n",
    "logpred_sample_file = pd.concat([test_ids,logpred_sample_file.project_is_approved], axis=1)\n",
    "logpred_sample_file.to_csv('Submissions/logistic_regression_submission.csv', index=False)\n",
    "\n",
    "# Cross Validation Score\n",
    "crossvalidation = KFold(10, random_state=1)\n",
    "scores = cross_val_score(model, X, y, \n",
    "                scoring = 'accuracy',\n",
    "                cv = crossvalidation, n_jobs =1)\n",
    "\n",
    "print ('Folds: %i,accuracy: %.2f std: %.2f' \n",
    "% (len(scores),np.mean(np.abs(scores)),np.std(scores)))\n",
    "\n",
    "logpred_sample_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Essay Model\n",
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T14:54:22.785272Z",
     "start_time": "2018-04-23T14:53:48.991267Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model Hyper Parameters\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "validation_split = 0.2\n",
    "\n",
    "# Tokenization, Sequences, Word Index, and Word Embeddings\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_edf.project_essay_1.values)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_edf.project_essay_1.values)\n",
    "word_index = tokenizer.word_index\n",
    "essay_one_train_data = pad_sequences(train_sequences, maxlen=maxlen)\n",
    "essay_one_train_data = pd.DataFrame(essay_one_train_data)\n",
    "essay_one_train_data = pd.concat([essay_one_train_data, train_df_classes], axis=1)\n",
    "essay_one_train_data.head(10)\n",
    "\n",
    "# Creating a list of Essay 1 features\n",
    "essay_one_features = essay_one_train_data.columns[:-1]\n",
    "\n",
    "# Converting DataFrame Values to Arrays\n",
    "input_train = essay_one_train_data[essay_one_features].values\n",
    "y_train = essay_one_train_data['project_is_approved'].values\n",
    "\n",
    "essay_one_train_data.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T18:21:59.043051Z",
     "start_time": "2018-04-20T18:21:59.040307Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame(sequences).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Essay Model: Recurrent RNN in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T18:21:59.220307Z",
     "start_time": "2018-04-20T18:21:59.044669Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Embedding(50000, 32))\n",
    "model.add(SimpleRNN(32)) \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc']) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T18:42:09.577796Z",
     "start_time": "2018-04-20T18:21:59.221879Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(input_train, y_train, epochs=3, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T18:42:10.040453Z",
     "start_time": "2018-04-20T18:42:09.579330Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T16:15:02.651571Z",
     "start_time": "2018-04-23T16:14:49.635461Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenization, Sequences, Word Index, and Word Embeddings\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(test_edf.project_essay_1.values)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_edf.project_essay_1.values)\n",
    "essay_one_test_data = pad_sequences(test_sequences, maxlen=maxlen)\n",
    "essay_one_test_data = pd.DataFrame(essay_one_test_data)\n",
    "essay_one_test_data.head(10)\n",
    "\n",
    "# Converting DataFrame Values to Arrays\n",
    "input_test = essay_one_test_data[essay_one_features].values\n",
    "\n",
    "essay_one_test_data.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T16:23:52.577368Z",
     "start_time": "2018-04-23T16:23:44.139158Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction on the test data\n",
    "essay_one_pred = pd.DataFrame(model.predict(input_test))\n",
    "essay_one_pred.columns = ['project_is_approved']\n",
    "\n",
    "# Export Essay One Prediction Sample Files\n",
    "essay_one_pred_sample_file = pd.concat([test_ids,essay_one_pred], axis=1)\n",
    "essay_one_pred_sample_file.to_csv('Submissions/Essay_one_predictions.csv', index=False)\n",
    "essay_one_pred_sample_file.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Removing the Last Layer of Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T18:45:05.971749Z",
     "start_time": "2018-04-20T18:45:05.784375Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "model.add(Embedding(50000, 32))\n",
    "model.add(SimpleRNN(32)) \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc']) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T18:45:05.989391Z",
     "start_time": "2018-04-20T18:45:05.973860Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(input_train, y_train, epochs=3, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Bidirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T14:54:22.791053Z",
     "start_time": "2018-04-23T14:54:22.786604Z"
    }
   },
   "outputs": [],
   "source": [
    "input_train.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T16:27:28.286881Z",
     "start_time": "2018-04-23T16:27:28.282855Z"
    }
   },
   "outputs": [],
   "source": [
    "input_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T16:30:58.884934Z",
     "start_time": "2018-04-23T16:30:58.881918Z"
    }
   },
   "outputs": [],
   "source": [
    "input_train = input_train[:5000]\n",
    "y_train = y_train[:5000]\n",
    "input_test = input_test[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T16:28:32.573971Z",
     "start_time": "2018-04-23T16:28:32.139968Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(50000, 32))\n",
    "model.add(layers.Bidirectional(layers.GRU(32),\n",
    "                              input_shape=(None,input_train.shape[-1],)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T16:30:22.894477Z",
     "start_time": "2018-04-23T16:28:32.716526Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(input_train, y_train, epochs=3, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T16:30:25.265947Z",
     "start_time": "2018-04-23T16:30:24.845649Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T16:34:22.083789Z",
     "start_time": "2018-04-23T16:33:57.442716Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction on test site\n",
    "essay_one_pred = pd.DataFrame(model.predict(input_test))\n",
    "essay_one_pred.columns = ['project_is_approved']\n",
    "essay_one_pred_sample_file = pd.concat([test_ids,essay_one_pred], axis=1)\n",
    "essay_one_pred_sample_file.to_csv('Submissions/bigru_predictions.csv', index=False)\n",
    "essay_one_pred_sample_file.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Essay Model Bi Directional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T22:00:32.928038Z",
     "start_time": "2018-04-23T22:00:32.673089Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cfa3ca2c6f7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0messay_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_edf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_edf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtrain_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_edf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "essay_train_data = pd.DataFrame()\n",
    "for col in train_edf.columns:\n",
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(train_edf[col].values)\n",
    "  train_sequences = tokenizer.texts_to_sequences(train_edf[col].values)\n",
    "  essay_train_data_append = pad_sequences(train_sequences, maxlen=maxlen)\n",
    "  essay_train_data_append = pd.DataFrame(essay_train_data_append)\n",
    "  essay_train_data = pd.concat([essay_train_data,essay_train_data_append], axis=1)\n",
    "  print(col)\n",
    "  \n",
    "print(essay_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T16:47:40.747681Z",
     "start_time": "2018-04-23T16:47:01.216018Z"
    }
   },
   "outputs": [],
   "source": [
    "essay_test_data = pd.DataFrame()\n",
    "for col in test_edf.columns:\n",
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(test_edf[col].values)\n",
    "  test_sequences = tokenizer.texts_to_sequences(test_edf[col].values)\n",
    "  essay_test_data_append = pad_sequences(test_sequences, maxlen=maxlen)\n",
    "  essay_test_data_append = pd.DataFrame(essay_test_data_append)\n",
    "  essay_test_data = pd.concat([essay_test_data,essay_test_data_append], axis=1)\n",
    "  print(col)\n",
    "  \n",
    "print(essay_test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T16:48:02.566857Z",
     "start_time": "2018-04-23T16:48:02.117873Z"
    }
   },
   "outputs": [],
   "source": [
    "essay_test_data.columns = list(range(0,essay_test_data.shape[1]))\n",
    "essay_test_data.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T16:54:06.955193Z",
     "start_time": "2018-04-23T16:54:03.606777Z"
    }
   },
   "outputs": [],
   "source": [
    "input_test = essay_test_data.values\n",
    "input_test = essay_train_data.values\n",
    "\n",
    "input_train = input_train[:5000]\n",
    "y_train = y_train[:5000]\n",
    "input_test = input_test[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:15:21.428944Z",
     "start_time": "2018-04-23T17:15:20.997340Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(50000, 32))\n",
    "model.add(layers.Bidirectional(layers.GRU(32),\n",
    "                              input_shape=(None,input_train.shape[-1],)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:18:22.221725Z",
     "start_time": "2018-04-23T17:15:22.478286Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(input_train, y_train, epochs=5, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:18:22.662055Z",
     "start_time": "2018-04-23T17:18:22.222965Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:22:10.278009Z",
     "start_time": "2018-04-23T17:22:10.063432Z"
    }
   },
   "outputs": [],
   "source": [
    "input_test = essay_test_data[essay_one_features].values\n",
    "input_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:22:11.801282Z",
     "start_time": "2018-04-23T17:22:11.765504Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction on test site\n",
    "essay_pred = pd.DataFrame(model.predict(input_test))\n",
    "essay_pred.columns = ['project_is_approved']\n",
    "essay_pred_sample_file = pd.concat([test_ids,essay_pred], axis=1)\n",
    "essay_pred_sample_file.to_csv('Submissions/bigru_predictions.csv', index=False)\n",
    "essay_pred_sample_file.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Predicition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:47:26.087749Z",
     "start_time": "2018-04-23T17:47:26.065673Z"
    }
   },
   "outputs": [],
   "source": [
    "train_jdf = pd.concat([top_data, logpred_proba_train], axis=1)\n",
    "test_jdf \n",
    "\n",
    "\n",
    "\n",
    "cnn_data = pd.concat([top_data, train_df_classes], axis=1)\n",
    "top_data.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:42:45.579381Z",
     "start_time": "2018-04-23T17:42:45.575497Z"
    }
   },
   "outputs": [],
   "source": [
    "top_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:49:16.489937Z",
     "start_time": "2018-04-23T17:49:16.485091Z"
    }
   },
   "outputs": [],
   "source": [
    "input_train = top_data.drop(['project_is_approved'], axis=1).values\n",
    "test_train =\n",
    "y_train = train_df_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T18:01:45.179443Z",
     "start_time": "2018-04-23T18:01:45.088028Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=(4,)))\n",
    "model.add(Dense(32, activation='relu',))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T18:02:00.814723Z",
     "start_time": "2018-04-23T18:01:46.054660Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(input_train, y_train, epochs=5, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T18:02:50.508025Z",
     "start_time": "2018-04-23T18:02:50.101421Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T18:05:14.471000Z",
     "start_time": "2018-04-23T18:05:14.457565Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction on test site\n",
    "cnn = pd.DataFrame(model.predict(input_test))\n",
    "cnn.columns = ['project_is_approved']\n",
    "cnn_sample_file = pd.concat([test_ids,cnn], axis=1)\n",
    "cnn_sample_file.to_csv('Submissions/cnn_predictions.csv', index=False)\n",
    "cnn_sample_file.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
